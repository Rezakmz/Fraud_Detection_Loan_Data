import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from numpy import genfromtxt
## 1) Loading & Splitting Data
df = pd.read_csv("application_data.csv")
X = df.drop('TARGET', axis=1)
y = df['TARGET']
from sklearn.model_selection import train_test_split

# 75 25 splitt and standardizing data
# stratify to balance labels in train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10, shuffle=True, stratify=y)
plt.hist(y_test)
plt.hist(y_train)
Highly Imbalanced data!!!
## 2) Missing Data
def percent_missing(df):
    percent_nan = 100* df.isnull().sum() / len(df)
    percent_nan = percent_nan[percent_nan>0].sort_values()
    return percent_nan
percent_nan = percent_missing(df)
plt.figure(figsize=(12, 8))
sns.barplot(x=percent_nan.index,y=percent_nan)
plt.xticks(rotation=90);
# replace the missing data with mode and median values
for column in X_train:
    
    if X_train[column].dtype == np.object_:
        X_train[column].fillna(X_train[column].mode()[0], inplace=True)
    else:
        X_train[column].fillna(X_train[column].median(), inplace=True)
# replace the missing data with mode and median values
for column in X_test:
    
    if X_test[column].dtype == np.object_:
        X_test[column].fillna(X_test[column].mode()[0], inplace=True)
    
    else:
        X_test[column].fillna(X_test[column].median(), inplace=True)
## 3) Feature Selection
from sklearn.preprocessing import LabelEncoder
import pickle

for column in X_train: 
    if X_train[column].dtype == np.object_:
        le = LabelEncoder()
        X_train[column] = le.fit_transform(X_train[column])
        X_test[column] = le.transform(X_test[column])
        output = open('Data\\'+
                      column+'_encoder', 'wb')
        pickle.dump(le, output)
        output.close()
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif

# feature selection
def select_features(X_train, y_train, X_test):
    fs = SelectKBest(score_func=mutual_info_classif, k=50)
    fs.fit(X_train, y_train)
    X_train_fs = fs.transform(X_train)
    X_test_fs = fs.transform(X_test)
    return X_train_fs, X_test_fs, fs

X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)

for i in range(len(fs.scores_)):
    print('Feature %d: %f' % (i, fs.scores_[i]))
# plot the scores
plt.figure(figsize=(12, 8))
plt.bar([i for i in range(len(fs.scores_))], fs.scores_)
plt.show()
## 4) Oversampling
from imblearn.over_sampling import SMOTE

oversample = SMOTE()

X_train_os, y_train_os = oversample.fit_resample(X_train_fs, y_train)
y_train_os.value_counts()
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_os = scaler.fit_transform(X_train_os)
X_test_fs = scaler.transform(X_test_fs)
## 5) Model Development with Hyperparameter Tuning
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

# finding the hyperparameters to avoid overfitting
param_grid = {
    "n_estimators": [5, 10, 20, 50],
    "max_depth": [1, 2, 3, 5, 10, 20],
}
result_dic = {
    "n_estimators": [],
    "max_depth": [],
    "train_f1": [],
    "test_f1": []
}
for n_est in param_grid['n_estimators']:
    for depth in param_grid['max_depth']:
    
        rf_model = RandomForestClassifier(n_estimators=n_est,
                                             criterion='entropy',
                                             max_depth=depth,
                                             random_state=1)
        rf_model.fit(X_train_os, y_train_os)
        y_pred = rf_model.predict(X_train_os)  
        predictions = rf_model.predict(X_test_fs)  

        f1_train = f1_score(y_pred, y_train_os)
        f1_test = f1_score(y_test, predictions)

        result_dic['n_estimators'].append(n_est)
        result_dic['max_depth'].append(depth)
        result_dic['train_f1'].append(f1_train)
        result_dic['test_f1'].append(f1_test)
    
plt.plot(result_dic['max_depth'], result_dic['train_f1'], c='blue')
plt.plot(result_dic['max_depth'], result_dic['test_f1'], c='orange')
## 6) Model Evaluation
from sklearn.metrics import confusion_matrix

rf_model = RandomForestClassifier(n_estimators=20,
                                     criterion='entropy',
                                     max_depth=2,
                                     random_state=1)

rf_model.fit(X_train_os, y_train_os) 

y_pred = rf_model.predict(X_train_os)
predictions = rf_model.predict(X_test_fs)  
conf_mat_test = confusion_matrix(y_test, predictions)
conf_mat_test
from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))
## 7) Optimum Threshold
# Searching for the best probability threshold
probs_1 = rf_model.predict_proba(X_test_fs)[:, 1]
thresholds = np.arange(0, 1.05, 0.05)
result_dic = {
'f1_scores' : [],
'thresholds' : []
}
predictions_new = [] 

for thresh in thresholds:
    for prob in probs_1:
        if prob >= thresh:
            predictions_new.append(1)
        else:
            predictions_new.append(0)
    f1 = f1_score(y_test, predictions_new)
    result_dic['f1_scores'].append(f1)
    result_dic['thresholds'].append(thresh)
    predictions_new = []
plt.xlabel('Threshold')
plt.ylabel('f1_score')
plt.plot(result_dic['thresholds'], result_dic['f1_scores'])
## 8) Feature Importance
from imblearn.over_sampling import SMOTE

oversample = SMOTE()

X_train_os, y_train_os = oversample.fit_resample(X_train, y_train)
rf_model = RandomForestClassifier(n_estimators=20,
                                     criterion='entropy',
                                     max_depth=2,
                                     random_state=1)

rf_model.fit(X_train_os, y_train_os) 

y_pred = rf_model.predict(X_train_os)
predictions = rf_model.predict(X_test)  
df_importance = pd.DataFrame(index=X_test.columns, data=rf_model.feature_importances_, columns=['importances'])
df_importance.sort_values('importances', ascending=True, inplace=True)
plt.figure(figsize=(16, 8))
sns.barplot(x=df_importance.index,y=df_importance['importances'])
plt.xticks(rotation=90);
df_importance.tail(10)
## 9) Model Development with Most Important Features
X_train_10 = X_train[df_importance.tail(10).index]
X_test_10 = X_test[df_importance.tail(10).index]

from imblearn.over_sampling import SMOTE

oversample = SMOTE()

X_train_10, y_train_10 = oversample.fit_resample(X_train_10, y_train)

from sklearn.metrics import confusion_matrix

rf_model = RandomForestClassifier(n_estimators=20,
                                     criterion='entropy',
                                     max_depth=2,
                                     random_state=1)

rf_model.fit(X_train_10, y_train_10) 

y_pred = rf_model.predict(X_train_10)
predictions = rf_model.predict(X_test_10)  
print(classification_report(y_test, predictions))
